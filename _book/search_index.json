[["index.html", "Reproducible Science Final Project Chapter 1 About", " Reproducible Science Final Project John Randolph 2023-04-20 Chapter 1 About This website is created to display and walk through the GPS cleaning process for collared mountain lion data. "],["creating-a-database.html", "Chapter 2 Creating a database 2.1 Establish the connection with database 2.2 Creating Tables", " Chapter 2 Creating a database Below I display how to create a database for my final project in reproducible data science for 2023. Provided is an example databse that we will be creating. Load all packages needed to create the database library(DBI) 2.1 Establish the connection with database library(DBI) Here we will establish the connection for the database to the directory using dbconnect Thesis_database &lt;- dbConnect(RSQLite::SQLite(), &quot;C:/Users/jaran/Desktop/R/Spr22Classwork/Reproducible Science/Final_Project/SQL_database/Thesis_Database.db&quot;) class(Thesis_database) 2.2 Creating Tables Below I will create a table using the dbexecute function. Within this function I will then use SQL language to create tables for my database. dbExecute(Thesis_database, &quot;CREATE TABLE Lion ( lion_id varchar(6) NOT NULL, sex char(1) CHECK (sex IN (&#39;M&#39;, &#39;F&#39;)), age char(2), capture_date text, capture_site varchar(25), PRIMARY KEY (lion_id));&quot;) dbExecute(Thesis_database, &quot;CREATE TABLE Capture ( lion_id varchar(6) NOT NULL, capture_id char(4), capture_site varchar(25), capture_date text, utm_x float, utm_y float, PRIMARY KEY (capture_id) FOREIGN KEY (capture_site) REFERENCES Lion(capture_site) FOREIGN KEY (lion_id) REFERENCES Lion(lion_id) FOREIGN KEY (capture_date) REFERENCES Lion(capture_date) );&quot;) dbExecute(Thesis_database, &quot;CREATE TABLE GPS_Collar ( lion_id varchar(6) NOT NULL, lionid_collarid varchar(25)), collar_id varchar(10), make varchar(25), model varchar(25), deployment_start text, deploymeny_end text, PRIMARY KEY (lionid_collarid) FOREIGN KEY (lion_id) REFERENCES Lion(lion_id) );&quot;) dbExecute(Thesis_database, &quot;CREATE TABLE GPS_Locaton ( locationid varchar(20) NOT NULL, lionid_collarid varchar(25), collar_id varchar(10), localtime text, utme_x float, utmn_y float, PRIMARY KEY (locationid) FOREIGN KEY (lionid_collarid) REFERENCES "],["processing-raw-data.html", "Chapter 3 Processing Raw Data", " Chapter 3 Processing Raw Data Load all packages needed to begin processing data. library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.2.3 ## Warning: package &#39;ggplot2&#39; was built under R version 4.2.3 ## Warning: package &#39;tibble&#39; was built under R version 4.2.3 ## Warning: package &#39;readr&#39; was built under R version 4.2.3 ## Warning: package &#39;dplyr&#39; was built under R version 4.2.3 ## ── Attaching core tidyverse packages ───────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.1 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ─────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract library(dplyr) library(purrr) library(lubridate) library(amt) ## ## Attaching package: &#39;amt&#39; ## ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(elevatr) library(sf) ## Warning: package &#39;sf&#39; was built under R version 4.2.3 ## Linking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE library(sp) ## ## Attaching package: &#39;sp&#39; ## ## The following object is masked from &#39;package:amt&#39;: ## ## bbox Next we will read the raw data in for one individual. The code is also piped to change all upper case to lower and to remove spaces Lion &lt;- read_csv(&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Raw_Data/IWS_Lions/FromDave/M280_GPS_Collar31756_20230112141501.csv&quot;, locale=locale(encoding=&quot;latin1&quot;)) %&gt;% rename_with(~ tolower(gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, .))) %&gt;% rename_with(~gsub(&quot; &quot;, &quot;&quot;, .)) ## New names: ## Rows: 3978 Columns: 52 ## ── Column specification ## ───────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (8): UTC_Date, LMT_Date, Origin, SCTS_Date, FixType, Mort. Status, ## Ani... dbl (14): No, CollarID, ECEF_X [m], ECEF_Y [m], ECEF_Z [m], ## Latitude [°], L... lgl (27): 3D_Error [m], Sats, Sat...20, C/N...21, ## Sat...22, C/N...23, Sat..... time (3): UTC_Time, LMT_Time, SCTS_Time ## ℹ Use `spec()` to retrieve the full column specification for this ## data. ℹ Specify the column types or set `show_col_types = FALSE` to ## quiet this message. ## • `Sat` -&gt; `Sat...20` ## • `C/N` -&gt; `C/N...21` ## • `Sat` -&gt; `Sat...22` ## • `C/N` -&gt; `C/N...23` ## • `Sat` -&gt; `Sat...24` ## • `C/N` -&gt; `C/N...25` ## • `Sat` -&gt; `Sat...26` ## • `C/N` -&gt; `C/N...27` ## • `Sat` -&gt; `Sat...28` ## • `C/N` -&gt; `C/N...29` ## • `Sat` -&gt; `Sat...30` ## • `C/N` -&gt; `C/N...31` ## • `Sat` -&gt; `Sat...32` ## • `C/N` -&gt; `C/N...33` ## • `Sat` -&gt; `Sat...34` ## • `C/N` -&gt; `C/N...35` ## • `Sat` -&gt; `Sat...36` ## • `C/N` -&gt; `C/N...37` ## • `Sat` -&gt; `Sat...38` ## • `C/N` -&gt; `C/N...39` ## • `Sat` -&gt; `Sat...40` ## • `C/N` -&gt; `C/N...41` ## • `Sat` -&gt; `Sat...42` ## • `C/N` -&gt; `C/N...43` Here we selected columns to work with and removed all NA’s in latitude and longitude. Here, I also shift my utctime to character so I can later convert it to GMT Lion %&lt;&gt;% dplyr::select(no, collarid, utcdate, utctime, latitude, longitude, dop, fixtype) %&gt;% filter(is.na(latitude) == F | is.na(longitude) == F) %&gt;% mutate(utctime = as.character(utctime)) Here we will paste the utcdate and utctime together and convert it into GMT time then remove the old utcdate and utctime columns Lion %&lt;&gt;% mutate(GMT = as.POSIXct(paste(utcdate, utctime, sep = &quot; &quot;), tryFormats = c(&quot;%m/%d/%Y %I:%M:%S %p&quot;, &quot;%m/%d/%y %H:%M:%S&quot;, &quot;%m/%d/%Y %H:%M:%S&quot;), tz=&quot;GMT&quot;)) %&gt;% dplyr::select(-c(utcdate, utctime)) Next we will convert the GMT time to localetime for the timezone this lion was collared in(Pacific). We convert and then lock the local time to GMT. Once complted we remove the column GMT. Lion %&lt;&gt;% mutate(localtime = with_tz(GMT, tz = &quot;Etc/GMT+8&quot;)) %&gt;% mutate(localtime = force_tz(localtime, tzone = &quot;GMT&quot;)) %&gt;% dplyr::select(-c(GMT)) Below I now filter the time the collar was on the mountain lion. I set this up to begin one day after collaring and end one day before the collar drop off mechanism is triggered. In this case the lion was collared on 11/17/2018 and the collar dropped on on 6/29/2022. Lion %&lt;&gt;% filter(localtime &gt;= &quot;2021/11/18&quot; &amp; localtime &lt;= &quot;2022/6/28&quot;) Now we will access the satellite dop ratings for each GPS point to filter thrown points. For my project we decided to remove any dop that is higher than 6. head(Lion$dop) ## [1] 1.4 1.2 1.4 1.4 1.2 1.2 max(Lion$dop) ## [1] 9.8 Lion %&lt;&gt;% filter(dop &lt;= 6) The last part of processing is to convert the latitude and longitude to utme and utmn. Below I transformed longitude and latitude to a simple feature spatial object and then to UTMs in WGS84. Lion %&lt;&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326, na.fail = FALSE, remove = FALSE) %&gt;% # transform to simple feature spatial object st_transform(32610) %&gt;% # transform to utms in WGS84 mutate(utme = unlist(map(geometry, 1)), utmn = unlist(map(geometry, 2))) %&gt;% st_drop_geometry() Finally inspect data that all conversions and filtering is correct and export the processed data to its new folder. names(Lion) ## [1] &quot;no&quot; &quot;collarid&quot; &quot;latitude&quot; &quot;longitude&quot; &quot;dop&quot; &quot;fixtype&quot; ## [7] &quot;localtime&quot; &quot;utme&quot; &quot;utmn&quot; head(Lion) ## # A tibble: 6 × 9 ## no collarid latitude longitude dop fixtype localtime utme ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 11033 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-28 05:00:13 6.51e5 ## 2 11032 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-28 03:00:24 6.51e5 ## 3 11031 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-28 01:00:13 6.51e5 ## 4 11030 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-27 23:00:38 6.51e5 ## 5 11029 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-27 21:00:18 6.51e5 ## 6 11028 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-27 19:00:12 6.47e5 ## # ℹ 1 more variable: utmn &lt;dbl&gt; summary(Lion$latitude) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 41.27 41.37 41.41 41.41 41.45 41.62 colnames(Lion) ## [1] &quot;no&quot; &quot;collarid&quot; &quot;latitude&quot; &quot;longitude&quot; &quot;dop&quot; &quot;fixtype&quot; ## [7] &quot;localtime&quot; &quot;utme&quot; &quot;utmn&quot; ncol(Lion) ## [1] 9 M280 &lt;- saveRDS(Lion, &quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/California/M280.rds&quot;) M280 &lt;- write.csv(Lion, file=&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/California/M280.csv&quot;) "],["data-resampling.html", "Chapter 4 Data Resampling", " Chapter 4 Data Resampling Load all packages needed to begin resampling. library(tidyverse) library(magrittr) library(dplyr) library(purrr) library(lubridate) library(amt) library(elevatr) library(sf) library(sp) Next we will read the processed data in for one individual and inspect data lion &lt;- readRDS(&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/California/Dop6/M280.rds&quot;) head(lion) ## # A tibble: 6 × 9 ## no collarid latitude longitude dop fixtype localtime utme ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 11033 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-28 05:00:13 6.51e5 ## 2 11032 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-28 03:00:24 6.51e5 ## 3 11031 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-28 01:00:13 6.51e5 ## 4 11030 31756 41.4 -121. 1.4 val. GPS-3D 2022-06-27 23:00:38 6.51e5 ## 5 11029 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-27 21:00:18 6.51e5 ## 6 11028 31756 41.4 -121. 1.2 val. GPS-3D 2022-06-27 19:00:12 6.47e5 ## # ℹ 1 more variable: utmn &lt;dbl&gt; Now we will transform the processed data into a track to begin our resampling process. trk &lt;- lion %&gt;% mk_track(.x = utme, .y = utmn, .t = localtime, id = collarid, crs = &quot;+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs&quot;, all_cols = TRUE) Next we will use track resample to convert our localtime to down sample to 4 hour points with 10 minute tolerance. trk %&lt;&gt;% track_resample(rate = hours(4), tolerance = minutes(10)) %&gt;% time_of_day(include.crepuscule = TRUE) Last we will check to see if the data was resampled propoerly then extract the data out of the track and into a new object. head(trk) ## # A tibble: 6 × 11 ## x_ y_ t_ no collarid latitude longitude dop ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 650736. 4595602. 2021-11-18 07:00:32 7096 31756 41.5 -121. 1.2 ## 2 650793. 4595520. 2021-11-18 11:00:16 7098 31756 41.5 -121. 1 ## 3 650854. 4595505. 2021-11-18 15:00:21 7100 31756 41.5 -121. 1.2 ## 4 651071. 4596019. 2021-11-18 19:00:12 7102 31756 41.5 -121. 1.2 ## 5 649567. 4594915. 2021-11-18 23:00:32 7104 31756 41.5 -121. 1.2 ## 6 649670. 4593700. 2021-11-19 03:00:16 7106 31756 41.5 -121. 1 ## # ℹ 3 more variables: fixtype &lt;chr&gt;, burst_ &lt;dbl&gt;, tod_ &lt;fct&gt; lion &lt;- trk %&gt;% dplyr::select(no, collarid, latitude, longitude, localtime = t_, utme = x_, utmn = y_, dop, fixtype) head(lion) ## # A tibble: 6 × 9 ## no collarid latitude longitude localtime utme utmn dop ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7096 31756 41.5 -121. 2021-11-18 07:00:32 650736. 4595602. 1.2 ## 2 7098 31756 41.5 -121. 2021-11-18 11:00:16 650793. 4595520. 1 ## 3 7100 31756 41.5 -121. 2021-11-18 15:00:21 650854. 4595505. 1.2 ## 4 7102 31756 41.5 -121. 2021-11-18 19:00:12 651071. 4596019. 1.2 ## 5 7104 31756 41.5 -121. 2021-11-18 23:00:32 649567. 4594915. 1.2 ## 6 7106 31756 41.5 -121. 2021-11-19 03:00:16 649670. 4593700. 1 ## # ℹ 1 more variable: fixtype &lt;chr&gt; Export data into a new folder to organize different resampled data. M280 &lt;- saveRDS(lion, &quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ResampledData/California/4hrs/M280.rds&quot;) M280 &lt;- write.csv(lion, file=&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ResampledData/California/4hrs/M280.csv&quot;, row.names = FALSE) "],["cluster-identifiation.html", "Chapter 5 Cluster Identifiation", " Chapter 5 Cluster Identifiation Load all packages needed to begin and source material. This code was created by Peter Mahoney and can be found at https://github.com/PMahoney29/rAnimalSiteFidelity library(bcpa) ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:amt&#39;: ## ## arrange, mutate, summarise, summarize ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact library(rgdal) ## Warning: package &#39;rgdal&#39; was built under R version 4.2.3 ## Please note that rgdal will be retired during 2023, ## plan transition to sf/stars/terra functions using GDAL and PROJ ## at your earliest convenience. ## See https://r-spatial.org/r/2022/04/12/evolution.html and https://github.com/r-spatial/evolution ## rgdal: version: 1.6-5, (SVN revision 1199) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.5.2, released 2022/09/02 ## Path to GDAL shared files: C:/Users/jaran/AppData/Local/R/win-library/4.2/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821] ## Path to PROJ shared files: C:/Users/jaran/AppData/Local/R/win-library/4.2/rgdal/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.6-0 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading sp or rgdal. library(rgeos) ## Warning: package &#39;rgeos&#39; was built under R version 4.2.3 ## rgeos version: 0.6-2, (SVN revision 693) ## GEOS runtime version: 3.9.3-CAPI-1.14.3 ## Please note that rgeos will be retired during 2023, ## plan transition to sf functions using GEOS at your earliest convenience. ## GEOS using OverlayNG ## Linking to sp version: 1.6-0 ## Polygon checking: TRUE ## ## Attaching package: &#39;rgeos&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## symdiff library(maptools) ## Checking rgeos availability: TRUE ## Please note that &#39;maptools&#39; will be retired during 2023, ## plan transition at your earliest convenience; ## some functionality will be moved to &#39;sp&#39;. library(foreach) ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when library(doParallel) ## Loading required package: iterators ## Loading required package: parallel library(lubridate) library(ggplot2) library(dplyr) library(raster) ## Warning: package &#39;raster&#39; was built under R version 4.2.3 ## ## Attaching package: &#39;raster&#39; ## The following object is masked from &#39;package:amt&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select source(&quot;rASF.R&quot;) ## Creating a generic function for &#39;GetVT&#39; from package &#39;bcpa&#39; in the global environment ## Creating a generic function for &#39;WindowSweep&#39; from package &#39;bcpa&#39; in the global environment Next we will read the resampled data in for one individual and convert the localtime to a specific format of mdy_hm. Next I removed duplicates from the data. d &lt;- read.csv(&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ResampledData/California/4hrs/M280.csv&quot;) d$dt &lt;- ymd_hms(d$localtime, tz = &#39;GMT&#39;) d &lt;- d[!duplicated(d$dt),] coordinates(d) &lt;- d[, c(&#39;longitude&#39;, &#39;latitude&#39;)] proj4string(d) &lt;- CRS(&#39;+proj=longlat +datum=WGS84&#39;) Now we will be sure to project for accurate measurements and note the zone of which the collared individual was collared. utmProj = CRS(&quot;+proj=utm +zone=10 +datum=WGS84&quot;) d &lt;- spTransform(d, utmProj) str(d) ## Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 1322 obs. of 10 variables: ## .. ..$ no : int [1:1322] 7096 7098 7100 7102 7104 7106 7108 7110 7112 7114 ... ## .. ..$ collarid : int [1:1322] 31756 31756 31756 31756 31756 31756 31756 31756 31756 31756 ... ## .. ..$ latitude : num [1:1322] 41.5 41.5 41.5 41.5 41.5 ... ## .. ..$ longitude: num [1:1322] -121 -121 -121 -121 -121 ... ## .. ..$ localtime: chr [1:1322] &quot;2021-11-18 07:00:32&quot; &quot;2021-11-18 11:00:16&quot; &quot;2021-11-18 15:00:21&quot; &quot;2021-11-18 19:00:12&quot; ... ## .. ..$ utme : num [1:1322] 650736 650793 650854 651071 649567 ... ## .. ..$ utmn : num [1:1322] 4595602 4595520 4595505 4596019 4594915 ... ## .. ..$ dop : num [1:1322] 1.2 1 1.2 1.2 1.2 1 1.2 1 1 1.2 ... ## .. ..$ fixtype : chr [1:1322] &quot;val. GPS-3D&quot; &quot;val. GPS-3D&quot; &quot;val. GPS-3D&quot; &quot;val. GPS-3D&quot; ... ## .. ..$ dt : POSIXct[1:1322], format: &quot;2021-11-18 07:00:32&quot; &quot;2021-11-18 11:00:16&quot; ... ## ..@ coords.nrs : num(0) ## ..@ coords : num [1:1322, 1:2] 650736 650793 650854 651071 649567 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:1322] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;longitude&quot; &quot;latitude&quot; ## ..@ bbox : num [1:2, 1:2] 642411 4571245 679611 4609502 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2] &quot;longitude&quot; &quot;latitude&quot; ## .. .. ..$ : chr [1:2] &quot;min&quot; &quot;max&quot; ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## .. .. ..@ projargs: chr &quot;+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs&quot; ## .. .. ..$ comment: chr &quot;PROJCRS[\\&quot;unknown\\&quot;,\\n BASEGEOGCRS[\\&quot;unknown\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n &quot;| __truncated__ Now running a function from the pulled github package to assign all data in a specific order. clustxy &lt;- ClusterXY(xy = coordinates(d), dt = d$dt, proj4string = utmProj, id = &#39;31756&#39;, PointID = &#39;no&#39;, Data = d@data) Next we assign the parameters to the cluster identifier. nfixes is the total number of points in the location to be considered a cluster. sbuffer is the radius around the cluster that points can be found in. tbuffer is the amount of time that has to past without a new point at the cluster for the cluster to end. intime, inPar, and nCores allow for smoother running of the identifier code. nfixes = 4 sbuffer = 150 tbuffer = 24 intime = T inPar = TRUE nCores = detectCores() - 1 Run code below to identify clusters with parameters above. M280_clusters &lt;- visualize_clusters(clustxy, nfixes, sbuffer, tbuffer, intime, inPar = inPar, nCores = nCores) summary(M280_clusters) ## $DateTime ## [1] &quot;2023-04-20 15:59:08 MDT&quot; ## ## $RunTime ## Time difference of 11.46737 secs ## ## $ClusterParameters ## ID nfixes sbuffer tbuffer intime inPar nCores ## 1 31756 4 150 24 TRUE TRUE 15 ## ## $SpatialComposition ## TotalClusters TotalPolygons TotalLines TotalPntsInClusters TotalOrigPoints ## 1 57 57 0 450 1322 Below I created a for loop to extract all the points in each cluster that was identified but leave the first and last one out. THis will allow me to have a starting and or end point for running an iSSA. temp &lt;- M280_clusters@Data[[1]] first &lt;- nrow(temp) last &lt;- 1 toremove &lt;- temp[-c(first,last),] # Forloop for rest of the dataset for (i in 2:length(M280_clusters@Data)){ temp &lt;- M280_clusters@Data[[i]] first &lt;- nrow(temp) last &lt;- 1 tempnew &lt;- temp[-c(first,last),] toremove &lt;- rbind(toremove, tempnew) } Lastly export all points that are provided from the for loop above. M280 &lt;- saveRDS(toremove, &quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ClustersforRemoval/M280ClusterRemoval.rds&quot;) M280 &lt;- write.csv(toremove, file=&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ClustersforRemoval/M280ClusterRemoval.csv&quot;, row.names = F) "],["data-resampling-1.html", "Chapter 6 Data Resampling", " Chapter 6 Data Resampling Load all packages needed to remove clusters. library(dplyr) library(magrittr) Next we will read the ressampled data and the cluster identified data in for one individual and inspect data ProssData &lt;- read.csv(&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ResampledData/California/4hrs/M280.csv&quot;) RemoveClust &lt;- read.csv(&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/ProcessedData/ClustersforRemoval/California/M280ClusterRemoval.csv&quot;) Next we will select PointID in the RemoveClust and change it to no so that both files have an identical column. RemoveClust &lt;- RemoveClust %&gt;% dplyr::select(no = &quot;PointID&quot;) Lastly, we will `anti_join the ProssData and RemoveClust by the no column to remove all cluster points from our data. CleanedData &lt;- anti_join(ProssData, RemoveClust, by = &quot;no&quot;) Finally export all data into a new folder called cleaned data and begin your analysis!!!! M280 &lt;- saveRDS(CleanedData, &quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/CleanedData/California/Dop6/M2804hrCleaned.rds&quot;) M280 &lt;- write.csv(CleanedData, file=&quot;C:/Users/jaran/Desktop/USU_Thesis_Project/Cleaned_Data/CleanedData/California/Dop6/M2804hrCleaned.csv&quot;, row.names = F) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
